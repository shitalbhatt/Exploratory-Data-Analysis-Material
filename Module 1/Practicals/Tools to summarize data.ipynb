{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d442b16-b973-4135-8f97-384a854ad142",
   "metadata": {},
   "source": [
    "\n",
    "# Tools to summarize data:\n",
    "o\tPivots, Filters, lookups\n",
    "Exploring Pandas in Python: Filter and Pivot Operations with Sample Data\n",
    "Pandas is a powerful data manipulation library in Python, and the filter and pivot functions are useful tools for working with DataFrame objects. Let's go through each of them using sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec7056c-29ab-43d7-9867-4a9d31eaaa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "         Date Category  Value\n",
      "0  2023-01-01        A     10\n",
      "1  2023-01-01        B     20\n",
      "2  2023-01-02        A     30\n",
      "3  2023-01-02        B     40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n",
    "    'Category': ['A', 'B', 'A', 'B'],\n",
    "    'Value': [10, 20, 30, 40]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e923-f3b3-4f4b-946a-d510b9597631",
   "metadata": {},
   "source": [
    "This creates a simple DataFrame with columns ‘Date’, ‘Category’, and ‘Value’. Now, let’s explore filter and pivot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5695ac0-0e3c-4670-8583-6ff651100d7d",
   "metadata": {},
   "source": [
    "Filter\n",
    "The filter function is used to subset the DataFrame based on column labels. You can use it to select specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588b044c-64b6-4d44-99a8-788df52bcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered DataFrame:\n",
      "         Date  Value\n",
      "0  2023-01-01     10\n",
      "1  2023-01-01     20\n",
      "2  2023-01-02     30\n",
      "3  2023-01-02     40\n"
     ]
    }
   ],
   "source": [
    "# Filtering columns\n",
    "filtered_df = df.filter(items=['Date', 'Value'])\n",
    "print(\"\\nFiltered DataFrame:\")\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d08d8e6c-0719-452c-ae3d-42634b60a440",
   "metadata": {},
   "source": [
    "This will result in a DataFrame containing only the ‘Date’ and ‘Value’ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94848cf6-3a6a-430f-8d62-4f3524405cdc",
   "metadata": {},
   "source": [
    "# Example 1: Filtering Rows Based on a Condition\n",
    "You can use filter to select rows that meet a specific condition. For example, let's filter rows where the 'Value' is greater than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c729746-9cd0-4f05-ae80-4cc5d4dd6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Rows:\n",
      "         Date Category  Value\n",
      "2  2023-01-02        A     30\n",
      "3  2023-01-02        B     40\n"
     ]
    }
   ],
   "source": [
    "# Filtering rows based on a condition\n",
    "filtered_rows = df[df['Value'] > 20]\n",
    "print(\"\\nFiltered Rows:\")\n",
    "print(filtered_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79bd6f-8dbd-401f-a21b-cdcc11bb228a",
   "metadata": {},
   "source": [
    "# Example 2: Using like Parameter\n",
    "The like parameter allows you to select columns based on partial string matching. For example, let's select columns that contain the string 'Dat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64889563-e83b-4638-960f-4fc2f794ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Columns:\n",
      "         Date\n",
      "0  2023-01-01\n",
      "1  2023-01-01\n",
      "2  2023-01-02\n",
      "3  2023-01-02\n"
     ]
    }
   ],
   "source": [
    "# Using the like parameter\n",
    "selected_columns = df.filter(like='Dat')\n",
    "print(\"\\nSelected Columns:\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe472e08-99ec-40d0-911e-677d5d3ec50d",
   "metadata": {},
   "source": [
    "# Example 3: Using regex Parameter\n",
    "The regex parameter enables you to use regular expressions to match column names. Let's select columns that start with 'C' or 'V'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a86fe6b-a44f-4ba8-a74d-0a94aff9e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Columns with Regex:\n",
      "  Category  Value\n",
      "0        A     10\n",
      "1        B     20\n",
      "2        A     30\n",
      "3        B     40\n"
     ]
    }
   ],
   "source": [
    "# Using the regex parameter\n",
    "selected_columns_regex = df.filter(regex='^C|^V')\n",
    "print(\"\\nSelected Columns with Regex:\")\n",
    "print(selected_columns_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47c4a3-44d6-4181-8293-6f851c593d31",
   "metadata": {},
   "source": [
    "# Example 4: Filtering Columns Based on a List\n",
    "You can use filter to select columns based on a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c4b3b8-cbba-409e-b775-d7594b7cb778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Columns with List:\n",
      "         Date  Value\n",
      "0  2023-01-01     10\n",
      "1  2023-01-01     20\n",
      "2  2023-01-02     30\n",
      "3  2023-01-02     40\n"
     ]
    }
   ],
   "source": [
    "# Filtering columns based on a list\n",
    "selected_columns_list = df.filter(items=['Date', 'Value'])\n",
    "print(\"\\nSelected Columns with List:\")\n",
    "print(selected_columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c16c1-422f-4eb8-a1e5-49de31aa601f",
   "metadata": {},
   "source": [
    "# Example 5: Using items Parameter\n",
    "The items parameter can be used to select columns based on a list of substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80de07cd-89aa-4cc5-967b-34f016e6b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Columns with Items:\n",
      "         Date\n",
      "0  2023-01-01\n",
      "1  2023-01-01\n",
      "2  2023-01-02\n",
      "3  2023-01-02\n"
     ]
    }
   ],
   "source": [
    "# Using the items parameter\n",
    "selected_columns_items = df.filter(items=['Date', 'Val'])\n",
    "print(\"\\nSelected Columns with Items:\")\n",
    "print(selected_columns_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944ac34-6517-4b2f-bd52-0fa935e586ca",
   "metadata": {},
   "source": [
    "These examples showcase different ways to use the filter function in Pandas for both column and row selection based on various conditions and criteria.\n",
    "\n",
    "# Pivot\n",
    "The pivot function is used to reshape the DataFrame by pivoting the values in one column into new columns. Let's pivot the data based on the 'Category' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d141d2c-7825-4449-bf99-2ebe0be37e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivoted DataFrame:\n",
      "Category     A   B\n",
      "Date              \n",
      "2023-01-01  10  20\n",
      "2023-01-02  30  40\n"
     ]
    }
   ],
   "source": [
    "# Pivoting the DataFrame\n",
    "pivot_df = df.pivot(index='Date', columns='Category', values='Value')\n",
    "print(\"\\nPivoted DataFrame:\")\n",
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b389e439-9b69-4a54-93bd-edb214d3d44c",
   "metadata": {},
   "source": [
    "This will create a new DataFrame where unique values in the ‘Category’ column become new columns, and the ‘Value’ column provides the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8167a-1dfa-45ae-8a6f-f5e6ff8b08e1",
   "metadata": {},
   "source": [
    "# Pivot Table\n",
    "If you have duplicate entries for a combination of index and columns in the pivot operation, you can use the pivot_table function with an aggregation function to handle the duplicates. Let's add another entry for '2023-01-01' and 'Category' 'A' to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c06cc2e-39d6-49bc-9bec-7119628ad39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table DataFrame:\n",
      "Category     A   B\n",
      "Date              \n",
      "2023-01-01  10  20\n",
      "2023-01-02  30  40\n"
     ]
    }
   ],
   "source": [
    "# Adding a duplicate entry\n",
    "#df = df.append({'Date': '2023-01-01', 'Category': 'A', 'Value': 15}, ignore_index=True)\n",
    "\n",
    "# Using pivot_table to handle duplicates\n",
    "pivot_table_df = df.pivot_table(index='Date', columns='Category', values='Value', aggfunc='sum')\n",
    "print(\"\\nPivot Table DataFrame:\")\n",
    "print(pivot_table_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f324a5cc-3a2c-4268-aab9-aad371984253",
   "metadata": {},
   "source": [
    "This will create a pivot table where the values are aggregated using the sum function for duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6590356-bccf-47c3-896c-2b5547b1cbe5",
   "metadata": {},
   "source": [
    "# Example 1: Multi-level Indexing\n",
    "You can create a multi-level index by passing a list of columns to the index parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d778a4d-76cf-4644-bf70-02f99be9a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-level Index Pivot:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [(2023-01-01, A), (2023-01-01, B), (2023-01-02, A), (2023-01-02, B)]\n"
     ]
    }
   ],
   "source": [
    "# Using pivot with multi-level indexing\n",
    "multi_level_pivot = df.pivot(index=['Date', 'Category'], columns='Value')\n",
    "print(\"\\nMulti-level Index Pivot:\")\n",
    "print(multi_level_pivot)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ef69034-d115-47f7-9c29-390974587509",
   "metadata": {},
   "source": [
    "This creates a DataFrame with a multi-level index based on ‘Date’ and ‘Category’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b304f9-5503-4075-88c8-e53292524cce",
   "metadata": {},
   "source": [
    "# Example 2: Handling Missing Values with fillna\n",
    "You can use the fillna function to replace missing values after pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5274f819-9978-43be-b031-90e3928469ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot with Missing Values Filled:\n",
      "Category     A   B\n",
      "Date              \n",
      "2023-01-01  10  20\n",
      "2023-01-02  30  40\n"
     ]
    }
   ],
   "source": [
    "# Adding a duplicate entry\n",
    "#df = df.append({'Date': '2023-01-01', 'Category': 'A', 'Value': 15}, ignore_index=True)\n",
    "\n",
    "# Using groupby and pivot_table to handle duplicates\n",
    "pivot_fillna = df.groupby(['Date', 'Category'])['Value'].sum().unstack(fill_value=0)\n",
    "print(\"\\nPivot with Missing Values Filled:\")\n",
    "print(pivot_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a750c8-8499-4dce-bfda-3c9d2cd211cc",
   "metadata": {},
   "source": [
    "# Example 3: Aggregating with aggfunc\n",
    "The aggfunc parameter allows you to specify an aggregation function for duplicate entries. Let's use the aggfunc parameter to calculate the average value for duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cf29a7-6665-4d99-9f1c-d9327982f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table with Average for Duplicate Entries:\n",
      "Category       A     B\n",
      "Date                  \n",
      "2023-01-01  10.0  20.0\n",
      "2023-01-02  30.0  40.0\n"
     ]
    }
   ],
   "source": [
    "# Using pivot_table to calculate the average for duplicate entries\n",
    "pivot_avg = df.pivot_table(index='Date', columns='Category', values='Value', aggfunc='mean')\n",
    "print(\"\\nPivot Table with Average for Duplicate Entries:\")\n",
    "print(pivot_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cb504-0fda-4125-8b98-f20482e6f55f",
   "metadata": {},
   "source": [
    "# Example 4: Resetting Index after Pivot\n",
    "To reset the index after pivoting, you need to use pivot_table with an aggregation function for duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f453a08-7d66-4a05-88d9-a22cd56d95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table with Reset Index:\n",
      "Category        Date   A   B\n",
      "0         2023-01-01  10  20\n",
      "1         2023-01-02  30  40\n"
     ]
    }
   ],
   "source": [
    "# Using pivot_table to handle duplicate entries and resetting index\n",
    "pivot_table_df = df.pivot_table(index='Date', columns='Category', values='Value', aggfunc='sum')\n",
    "pivot_reset_index = pivot_table_df.reset_index()\n",
    "print(\"\\nPivot Table with Reset Index:\")\n",
    "print(pivot_reset_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a61b0-4ca9-4f90-8984-7d9bea386519",
   "metadata": {},
   "source": [
    "# Example 5: Pivoting with Multiple Value Columns\n",
    "You can pivot on multiple value columns by specifying a list for the values parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3490532b-ab5d-44b5-b527-e14b1059c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot with Multiple Value Columns:\n",
      "           Value     Value2     \n",
      "Category       A   B      A    B\n",
      "Date                            \n",
      "2023-01-01    10  20    100  200\n",
      "2023-01-02    30  40    300  400\n"
     ]
    }
   ],
   "source": [
    "# Sample Data with an additional 'Value2' column\n",
    "data = {\n",
    "    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n",
    "    'Category': ['A', 'B', 'A', 'B'],\n",
    "    'Value': [10, 20, 30, 40],\n",
    "    'Value2': [100, 200, 300, 400]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivoting with multiple value columns\n",
    "multi_value_pivot = df.pivot(index='Date', columns='Category', values=['Value', 'Value2'])\n",
    "print(\"\\nPivot with Multiple Value Columns:\")\n",
    "print(multi_value_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdcddd-43a8-4ca0-82b9-45d00416d5e3",
   "metadata": {},
   "source": [
    "# How to Do a vLookup in Python using pandas:\n",
    "Vlookup is essentially used for vertically arranged data. Vlookup is an operation used to merge 2 different data tables based on some condition where there must be at least 1 common attribute(column) between the two tables. After performing this operation we get a table consisting of all the data from both the tables for which the data is matched.\n",
    "We can use merge() function to perform Vlookup in pandas. The merge function does the same job as the Join in SQL We can perform the merge operation with respect to table 1 or table 2.There can be different ways of merging the 2 tables.\n",
    "Let’s consider 2 tables on which the operation is to be performed. 1st table consists of the information of students and 2nd column consists of the information of the respective Courses they are enrolled in. The below code tells the information contained in both the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20fcde8e-6816-431c-84f2-c7bdd000f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year Industry_aggregation_NZSIOC Industry_code_NZSIOC  \\\n",
      "0      2021                     Level 1                99999   \n",
      "1      2021                     Level 1                99999   \n",
      "2      2021                     Level 1                99999   \n",
      "3      2021                     Level 1                99999   \n",
      "4      2021                     Level 1                99999   \n",
      "...     ...                         ...                  ...   \n",
      "41710  2013                     Level 3                 ZZ11   \n",
      "41711  2013                     Level 3                 ZZ11   \n",
      "41712  2013                     Level 3                 ZZ11   \n",
      "41713  2013                     Level 3                 ZZ11   \n",
      "41714  2013                     Level 3                 ZZ11   \n",
      "\n",
      "             Industry_name_NZSIOC               Units Variable_code  \\\n",
      "0                  All industries  Dollars (millions)           H01   \n",
      "1                  All industries  Dollars (millions)           H04   \n",
      "2                  All industries  Dollars (millions)           H05   \n",
      "3                  All industries  Dollars (millions)           H07   \n",
      "4                  All industries  Dollars (millions)           H08   \n",
      "...                           ...                 ...           ...   \n",
      "41710  Food product manufacturing          Percentage           H37   \n",
      "41711  Food product manufacturing          Percentage           H38   \n",
      "41712  Food product manufacturing          Percentage           H39   \n",
      "41713  Food product manufacturing          Percentage           H40   \n",
      "41714  Food product manufacturing          Percentage           H41   \n",
      "\n",
      "                                         Variable_name      Variable_category  \\\n",
      "0                                         Total income  Financial performance   \n",
      "1      Sales, government funding, grants and subsidies  Financial performance   \n",
      "2                    Interest, dividends and donations  Financial performance   \n",
      "3                                 Non-operating income  Financial performance   \n",
      "4                                    Total expenditure  Financial performance   \n",
      "...                                                ...                    ...   \n",
      "41710                                      Quick ratio       Financial ratios   \n",
      "41711              Margin on sales of goods for resale       Financial ratios   \n",
      "41712                                 Return on equity       Financial ratios   \n",
      "41713                           Return on total assets       Financial ratios   \n",
      "41714                            Liabilities structure       Financial ratios   \n",
      "\n",
      "         Value                             Industry_code_ANZSIC06  \n",
      "0      757,504  ANZSIC06 divisions A-S (excluding classes K633...  \n",
      "1      674,890  ANZSIC06 divisions A-S (excluding classes K633...  \n",
      "2       49,593  ANZSIC06 divisions A-S (excluding classes K633...  \n",
      "3       33,020  ANZSIC06 divisions A-S (excluding classes K633...  \n",
      "4      654,404  ANZSIC06 divisions A-S (excluding classes K633...  \n",
      "...        ...                                                ...  \n",
      "41710       52  ANZSIC06 groups C111, C112, C113, C114, C115, ...  \n",
      "41711       40  ANZSIC06 groups C111, C112, C113, C114, C115, ...  \n",
      "41712       12  ANZSIC06 groups C111, C112, C113, C114, C115, ...  \n",
      "41713        5  ANZSIC06 groups C111, C112, C113, C114, C115, ...  \n",
      "41714       46  ANZSIC06 groups C111, C112, C113, C114, C115, ...  \n",
      "\n",
      "[41715 rows x 10 columns]\n",
      "       year industry_code_ANZSIC               industry_name_ANZSIC  \\\n",
      "0      2011                    A  Agriculture, Forestry and Fishing   \n",
      "1      2011                    A  Agriculture, Forestry and Fishing   \n",
      "2      2011                    A  Agriculture, Forestry and Fishing   \n",
      "3      2011                    A  Agriculture, Forestry and Fishing   \n",
      "4      2011                    A  Agriculture, Forestry and Fishing   \n",
      "...     ...                  ...                                ...   \n",
      "17023  2021                  all                     All Industries   \n",
      "17024  2021                  all                     All Industries   \n",
      "17025  2021                  all                     All Industries   \n",
      "17026  2021                  all                     All Industries   \n",
      "17027  2021                  all                     All Industries   \n",
      "\n",
      "        rme_size_grp                                         variable  \\\n",
      "0                a_0                                    Activity unit   \n",
      "1                a_0                           Rolling mean employees   \n",
      "2                a_0                          Salaries and wages paid   \n",
      "3                a_0  Sales, government funding, grants and subsidies   \n",
      "4                a_0                                     Total income   \n",
      "...              ...                                              ...   \n",
      "17023  j_Grand_Total                                     Total income   \n",
      "17024  j_Grand_Total                                Total expenditure   \n",
      "17025  j_Grand_Total                      Operating profit before tax   \n",
      "17026  j_Grand_Total                                     Total assets   \n",
      "17027  j_Grand_Total                            Fixed tangible assets   \n",
      "\n",
      "         value               unit  \n",
      "0        46134              COUNT  \n",
      "1            0              COUNT  \n",
      "2          279  DOLLARS(millions)  \n",
      "3         8187  DOLLARS(millions)  \n",
      "4         8866  DOLLARS(millions)  \n",
      "...        ...                ...  \n",
      "17023   757504  DOLLARS(millions)  \n",
      "17024   654404  DOLLARS(millions)  \n",
      "17025    85116  DOLLARS(millions)  \n",
      "17026  2512677  DOLLARS(millions)  \n",
      "17027   591351  DOLLARS(millions)  \n",
      "\n",
      "[17028 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('annual-enterprise-survey-2021-financial-year-provisional-csv.csv') \n",
    "df2 = pd.read_csv('annual-enterprise-survey-2021-financial-year-provisional-size-bands-csv.csv') \n",
    "\n",
    "print(df1) \n",
    "print(df2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd33bd-3714-4e1b-8a14-28baaf1775db",
   "metadata": {},
   "source": [
    "# Performing a Vlook on different types of Joins\n",
    " \n",
    "\n",
    "Inner join: Inner join produces an output data frame of only those rows for which the condition is satisfied in both the rows. To perform inner join you may specify inner as a keyword in how.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d538b778-9cd6-4249-89aa-04de61f038c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BRANCH_NM\\t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      6\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborrower.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 8\u001b[0m Left_join \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df1, \n\u001b[0;32m      9\u001b[0m \t\t\t\t\tdf2, \n\u001b[0;32m     10\u001b[0m \t\t\t\t\ton \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOAN_NO\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     11\u001b[0m \t\t\t\t\thow \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRANCH_NM\t\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     12\u001b[0m Left_join\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    170\u001b[0m         left_df,\n\u001b[0;32m    171\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:883\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 883\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m    885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    886\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1133\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1130\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1131\u001b[0m     )\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_indexers()\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1105\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_join_indexers(\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1718\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1717\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sort\n\u001b[1;32m-> 1718\u001b[0m join_func \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39minner_join,\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join,\n\u001b[0;32m   1721\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y, count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join(\n\u001b[0;32m   1722\u001b[0m         y, x, count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1723\u001b[0m     )[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join,\n\u001b[0;32m   1725\u001b[0m }[how]\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m join_func(lkey, rkey, count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'BRANCH_NM\\t'"
     ]
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv') \n",
    "\n",
    "Left_join = pd.merge(df1, \n",
    "\t\t\t\t\tdf2, \n",
    "\t\t\t\t\ton ='LOAN_NO', \n",
    "\t\t\t\t\thow ='BRANCH_NM\t') \n",
    "Left_join \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07cd4ac4-8451-44fd-8fb4-f602c5c7cb63",
   "metadata": {},
   "source": [
    "This code performs the following:\n",
    "\n",
    "Imports pandas library: This library provides tools for data analysis and manipulation.\n",
    "Creates sample data: A DataFrame is created with columns 'Product', 'Price', and 'Customer'.\n",
    "PivotTable: Groups data by 'Product' and calculates the sum of 'Price' for each product category.\n",
    "Filter: Creates a new DataFrame containing rows where 'Price' is greater than 5.\n",
    "Lookup: Defines a function price_category to assign a category ('Low', 'Medium', or 'High') based on the price range. A new column 'Category' is added to the DataFrame using the apply function and the defined function.\n",
    "This demonstrates how Pandas can be used for data summarization tasks using Pivots, Filters, and Lookups. You can adapt this code to your specific data and desired summarization requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3f419a-66ad-477d-a0b0-b3321771ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PivotTable (Product wise sum of Price): \n",
      "          Price\n",
      "Product       \n",
      "Book        30\n",
      "Pen          5\n",
      "Pencil       3\n",
      "\n",
      "Filtered data (Price > 5): \n",
      "   Product  Price Customer\n",
      "0    Book     10    Alice\n",
      "2    Book     20  Charlie\n",
      "\n",
      "Lookup (Price Category): \n",
      "   Product  Price Customer Category\n",
      "0    Book     10    Alice   Medium\n",
      "1  Pencil      1      Bob      Low\n",
      "2    Book     20  Charlie     High\n",
      "3     Pen      5    David      Low\n",
      "4  Pencil      2    Alice      Low\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4492\\3959552780.py:11: FutureWarning: The provided callable <built-in function sum> is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  product_pivot = df.pivot_table(values='Price', index='Product', aggfunc=sum)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Product': ['Book', 'Pencil', 'Book', 'Pen', 'Pencil'],\n",
    "        'Price': [10, 1, 20, 5, 2],\n",
    "        'Customer': ['Alice', 'Bob', 'Charlie', 'David', 'Alice']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# PivotTable (Summarize by Product)\n",
    "product_pivot = df.pivot_table(values='Price', index='Product', aggfunc=sum)\n",
    "print(\"PivotTable (Product wise sum of Price): \\n\", product_pivot)\n",
    "\n",
    "# Filter (Price greater than 5)\n",
    "filtered_df = df[df['Price'] > 5]\n",
    "print(\"\\nFiltered data (Price > 5): \\n\", filtered_df)\n",
    "\n",
    "# Lookup (Add 'Category' column based on price range)\n",
    "def price_category(price):\n",
    "  if price <= 5:\n",
    "    return 'Low'\n",
    "  elif price <= 10:\n",
    "    return 'Medium'\n",
    "  else:\n",
    "    return 'High'\n",
    "\n",
    "df['Category'] = df['Price'].apply(price_category)\n",
    "print(\"\\nLookup (Price Category): \\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84c47c-5f53-4dd1-b673-ce24f31b85d5",
   "metadata": {},
   "source": [
    "# How to Do a vLookup in Python using pandas\n",
    "Last Updated : 06 Aug, 2021\n",
    "Vlookup is essentially used for vertically arranged data. Vlookup is an operation used to merge 2 different data tables based on some condition where there must be at least 1 common attribute(column) between the two tables. After performing this operation we get a table consisting of all the data from both the tables for which the data is matched.\n",
    "We can use merge() function to perform Vlookup in pandas. The merge function does the same job as the Join in SQL We can perform the merge operation with respect to table 1 or table 2.There can be different ways of merging the 2 tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2377be-135e-43b1-b2c3-db1531b32007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a8fab2-3e1d-4036-a988-c609a2e7aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LOAN_NO BRANCH_NM  AMOUNT\n",
      "0    l-02      HDFC  100000\n",
      "1    l-01       SBI  200000\n",
      "2    l-03       BOI  400000\n",
      "3    l-04      HDFC  500000\n",
      "4    l-05      Axis  600000\n",
      "  CUSTOMER_NM LOAN_NO\n",
      "0           C    l-05\n",
      "1           B    l-03\n",
      "2           A    l-01\n"
     ]
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv') \n",
    "\n",
    "print(df1) \n",
    "print(df2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a1e03-a2a6-4302-8603-efe982fce350",
   "metadata": {},
   "source": [
    "# Performing a Vlook on different types of Joins\n",
    " \n",
    "\n",
    "Inner join: Inner join produces an output data frame of only those rows for which the condition is satisfied in both the rows. To perform inner join you may specify inner as a keyword in how.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40b146d-c78d-454b-acee-26ae6eddc8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_NO</th>\n",
       "      <th>BRANCH_NM</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>CUSTOMER_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l-01</td>\n",
       "      <td>SBI</td>\n",
       "      <td>200000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l-03</td>\n",
       "      <td>BOI</td>\n",
       "      <td>400000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l-05</td>\n",
       "      <td>Axis</td>\n",
       "      <td>600000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOAN_NO BRANCH_NM  AMOUNT CUSTOMER_NM\n",
       "0    l-01       SBI  200000           A\n",
       "1    l-03       BOI  400000           B\n",
       "2    l-05      Axis  600000           C"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv') \n",
    "\n",
    "inner_join = pd.merge(df1, \n",
    "\t\t\t\t\tdf2, \n",
    "\t\t\t\t\ton ='LOAN_NO', \n",
    "\t\t\t\t\thow ='inner') \n",
    "inner_join \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e998cc7b-1e61-461c-b6f2-b933fe7a772d",
   "metadata": {},
   "source": [
    "Left join: Left join operation provides all the rows from 1st dataframe and matching rows from the 2nd dataframe. If the rows are not matched in the 2nd dataframe then they will be replaced by NaN.\n",
    "Example:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbabfd1d-9385-4008-b933-a5da3660eff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_NO</th>\n",
       "      <th>BRANCH_NM</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>CUSTOMER_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l-02</td>\n",
       "      <td>HDFC</td>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l-01</td>\n",
       "      <td>SBI</td>\n",
       "      <td>200000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l-03</td>\n",
       "      <td>BOI</td>\n",
       "      <td>400000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l-04</td>\n",
       "      <td>HDFC</td>\n",
       "      <td>500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l-05</td>\n",
       "      <td>Axis</td>\n",
       "      <td>600000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOAN_NO BRANCH_NM  AMOUNT CUSTOMER_NM\n",
       "0    l-02      HDFC  100000         NaN\n",
       "1    l-01       SBI  200000           A\n",
       "2    l-03       BOI  400000           B\n",
       "3    l-04      HDFC  500000         NaN\n",
       "4    l-05      Axis  600000           C"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv') \n",
    "\n",
    "Left_join = pd.merge(df1, \n",
    "\t\t\t\t\tdf2, \n",
    "\t\t\t\t\ton ='LOAN_NO', \n",
    "\t\t\t\t\thow ='left') \n",
    "Left_join \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "208c8ea8-6325-475d-817f-78652a232391",
   "metadata": {},
   "source": [
    "Right join: Right join is somewhat similar to left join in which the output dataframe will consist of all the rows from the 2nd dataframe and matching rows from the 1st dataframe. If the rows are not matched in 1st row then they will be replaced by NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f80d6a0-1a63-4583-a428-0a57d40310f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_NO</th>\n",
       "      <th>BRANCH_NM</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>CUSTOMER_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l-05</td>\n",
       "      <td>Axis</td>\n",
       "      <td>600000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l-03</td>\n",
       "      <td>BOI</td>\n",
       "      <td>400000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l-01</td>\n",
       "      <td>SBI</td>\n",
       "      <td>200000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOAN_NO BRANCH_NM  AMOUNT CUSTOMER_NM\n",
       "0    l-05      Axis  600000           C\n",
       "1    l-03       BOI  400000           B\n",
       "2    l-01       SBI  200000           A"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv') \n",
    "\n",
    "Right_join = pd.merge(df1, \n",
    "\t\t\t\t\tdf2, \n",
    "\t\t\t\t\ton ='LOAN_NO', \n",
    "\t\t\t\t\thow ='right') \n",
    "Right_join \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa514fec-c6f6-4346-8418-14c02abcb514",
   "metadata": {},
   "source": [
    "Outer join: Outer join provides the output dataframe consisting of rows from both the dataframes. Values will be shown if rows are matched otherwise NaN will be shown for rows that do not match.\n",
    "Example:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2aeeccd-232a-4a0b-80fa-6870f8bf3e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN_NO</th>\n",
       "      <th>BRANCH_NM</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>CUSTOMER_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l-02</td>\n",
       "      <td>HDFC</td>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l-01</td>\n",
       "      <td>SBI</td>\n",
       "      <td>200000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l-03</td>\n",
       "      <td>BOI</td>\n",
       "      <td>400000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l-04</td>\n",
       "      <td>HDFC</td>\n",
       "      <td>500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l-05</td>\n",
       "      <td>Axis</td>\n",
       "      <td>600000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOAN_NO BRANCH_NM  AMOUNT CUSTOMER_NM\n",
       "0    l-02      HDFC  100000         NaN\n",
       "1    l-01       SBI  200000           A\n",
       "2    l-03       BOI  400000           B\n",
       "3    l-04      HDFC  500000         NaN\n",
       "4    l-05      Axis  600000           C"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas \n",
    "import pandas as pd \n",
    "\n",
    "# read csv data \n",
    "df1 = pd.read_csv('loan.csv') \n",
    "df2 = pd.read_csv('borrower.csv')  \n",
    "\n",
    "Outer_join = pd.merge(df1, \n",
    "\t\t\t\t\tdf2, \n",
    "\t\t\t\t\ton ='LOAN_NO', \n",
    "\t\t\t\t\thow ='outer') \n",
    "Outer_join \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6aa49f-d84d-4556-a2a2-cda87e1b7430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
